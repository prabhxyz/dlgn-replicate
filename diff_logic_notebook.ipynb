{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f985fb43",
   "metadata": {},
   "source": [
    "# Differentiable Logic Gate Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae45a199",
   "metadata": {},
   "source": [
    "This notebook is minimal yet flexible implementation of differentiable logic gates with a small example that learns XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df3dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- imports ----\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b85d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- differentiable gates ----\n",
    "class DiffGate(nn.Module):\n",
    "    \"\"\"Base gate: weighted sum -> sigmoid (learnable bias + temperature)\"\"\"\n",
    "    def __init__(self, bias):\n",
    "        super().__init__()\n",
    "        self.bias = nn.Parameter(torch.tensor(bias, dtype=torch.float))\n",
    "        self.scale = nn.Parameter(torch.ones(1))  # temperature\n",
    "\n",
    "    def forward(self, *xs):\n",
    "        s = torch.stack(xs, dim=-1).sum(-1)\n",
    "        return torch.sigmoid(self.scale * (s + self.bias))\n",
    "\n",
    "class AND(DiffGate):\n",
    "    def __init__(self):\n",
    "        super().__init__(bias=-1.5)\n",
    "\n",
    "class OR(DiffGate):\n",
    "    def __init__(self):\n",
    "        super().__init__(bias=-0.5)\n",
    "\n",
    "class NAND(DiffGate):\n",
    "    def __init__(self):\n",
    "        super().__init__(bias=1.5)\n",
    "\n",
    "class NOT(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 1 - x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77371d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- logic layer & network ----\n",
    "class LogicLayer(nn.Module):\n",
    "    \"\"\"Parallel stack of gates with soft wiring\"\"\"\n",
    "    def __init__(self, n_in, n_out, gate_type=AND):\n",
    "        super().__init__()\n",
    "        self.gates = nn.ModuleList([gate_type() for _ in range(n_out)])\n",
    "        self.w = nn.Parameter(torch.randn(n_out, n_in))\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs = []\n",
    "        for g, w_row in zip(self.gates, self.w):\n",
    "            y = g(*(x * w_row.sigmoid()))\n",
    "            outs.append(y)\n",
    "        return torch.stack(outs, dim=-1)\n",
    "\n",
    "class LogicNet(nn.Module):\n",
    "    def __init__(self, in_dim, hidden, out_dim):\n",
    "        super().__init__()\n",
    "        self.l1 = LogicLayer(in_dim, hidden, AND)\n",
    "        self.l2 = LogicLayer(hidden, out_dim, OR)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.l1(x)\n",
    "        y = self.l2(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d186b7eb",
   "metadata": {},
   "source": [
    "## Train on XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a2912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[0.,0.], [0.,1.], [1.,0.], [1.,1.]])\n",
    "Y = torch.tensor([[0.], [1.], [1.], [0.]])\n",
    "\n",
    "net = LogicNet(in_dim=2, hidden=4, out_dim=1)\n",
    "opt = torch.optim.Adam(net.parameters(), lr=0.05)\n",
    "\n",
    "for step in range(5000):\n",
    "    pred = net(X)\n",
    "    loss = F.mse_loss(pred, Y)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if step % 500 == 0:\n",
    "        print(f\"step {step}: loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fad194",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rounded predictions:', net(X).detach().round())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9346d9f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
